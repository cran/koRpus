#' Analyze lexical diversity
#' 
#' This function analyzes the lexical diversity/complexity of a text corpus.
#'
#' \code{lex.div} calculates a variety of proposed indices for lexical diversity. In the following formulae, \eqn{N} refers to
#' the total number of tokens, and \eqn{V} to the number of types:
#' \describe{
#'	\item{\code{"TTR"}:}{The ordinary \emph{Type-Token Ratio}: \deqn{TTR = \frac{V}{N}}{TTR =  V / N}
#'		Wrapper function: \code{\link[koRpus:TTR]{TTR}}}
#'	\item{\code{"MSTTR"}:}{For the \emph{Mean Segmental Type-Token Ratio} (sometimes referred to as \emph{Split TTR}) tokens are split up into 
#'		segments of the given size, TTR for each segment is calculated and the mean of these values returned. Tokens at the end which do 
#'		not make a full segment are ignored. The number of dropped tokens is reported.
#'
#'		Wrapper function: \code{\link[koRpus:MSTTR]{MSTTR}}}
#'	\item{\code{"MATTR"}:}{The \emph{Moving-Average Type-Token Ratio} (Covington & McFall, 2010) calculates TTRs for a defined number of tokens
#'		(called the "window"), starting at the beginning of the text and moving this window over the text, until the last token is reached.
#'		The mean of these TTRs is the MATTR.
#'
#'		Wrapper function: \code{\link[koRpus:MATTR]{MATTR}}}
#'	\item{\code{"C"}:}{Herdan's \emph{C} (Herdan, 1960, as cited in Tweedie & Baayen, 1998; sometimes referred to as \emph{LogTTR}): \deqn{C = \frac{\lg{V}}{\lg{N}}}{C = lg(V) / lg(N)}}
#'
#'		Wrapper function: \code{\link[koRpus:C.ld]{C.ld}}
#'	\item{\code{"R"}:}{Guiraud's \emph{Root TTR} (Guiraud, 1954, as cited in Tweedie & Baayen, 1998): \deqn{R = \frac{V}{\sqrt{N}}}{R = V / sqrt(N)}}
#'
#'		Wrapper function: \code{\link[koRpus:R.ld]{R.ld}}
#'	\item{\code{"CTTR"}:}{Carroll's \emph{Corrected TTR}: \deqn{CTTR = \frac{V}{\sqrt{2N}}}{CTTR = V / sqrt(2N)}}
#'
#'		Wrapper function: \code{\link[koRpus:CTTR]{CTTR}}
#'	\item{\code{"U"}:}{Dugast's \emph{Uber Index}  (Dugast, 1978, as cited in Tweedie & Baayen, 1998): \deqn{U = \frac{(\lg{N})^2}{\lg{N} - \lg{V}}}{U = lg(N)^2 / lg(N) - lg(V)}}
#'
#'		Wrapper function: \code{\link[koRpus:U.ld]{U.ld}}
#'	\item{\code{"S"}:}{Summer's index: \deqn{S = \frac{\lg{\lg{V}}}{\lg{\lg{N}}}}{S = lg(lg(V)) / lg(lg(N))}}
#'
#'		Wrapper function: \code{\link[koRpus:S.ld]{S.ld}}
#'	\item{\code{"K"}:}{Yule's \emph{K}  (Yule, 1944, as cited in Tweedie & Baayen, 1998) is calculated by: \deqn{K = 10^4 \times \frac{(\sum_{X=1}^{X}{{f_X}X^2}) - N}{N^2}}{K = 10^4 * (sum(fX*X^2) - N) / N^2}
#'		where \eqn{N} is the number of tokens, \eqn{X} is a vector with the frequencies of each type, and \eqn{f_X}{fX} is
#'		the frequencies for each X.
#'
#'		Wrapper function: \code{\link[koRpus:K.ld]{K.ld}}}
#'	\item{\code{"Maas"}:}{Maas' indices (\eqn{a}, \eqn{\lg{V_0}} & \eqn{\lg{}_{e}{V_0}}): \deqn{a^2 = \frac{\lg{N} - \lg{V}}{\lg{N}^2}}{a^2 = lg(N) - lg(V) / lg(N)^2}
#'  \deqn{\lg{V_0} = \frac{\lg{V}}{\sqrt{1 - \frac{\lg{V}}{\lg{N}}^2}}}{lg(V0) = lg(V) / sqrt(1 - (lg(V) / lg(N)^2))}
#'		Earlier versions (\code{koRpus} < 0.04-12) reported \eqn{a^2}, and not \eqn{a}. The measure was derived from a formula by Müller (1969, as cited in Maas, 1972).
#'		\eqn{\lg{}_{e}{V_0}} is equivalent to \eqn{\lg{V_0}}, only with \eqn{e} as the base for the logarithms. Also calculated are \eqn{a}, \eqn{\lg{V_0}} (both not the same
#'		as before) and \eqn{V'} as measures of relative vocabulary growth while the text progresses. To calculate these measures, the first half of the text and the full text
#'		will be examined (see Maas, 1972, p. 67 ff. for details).
#'
#'		Wrapper function: \code{\link[koRpus:maas]{maas}}}
#'	\item{\code{"MTLD"}:}{For the \emph{Measure of Textual Lexical Diversity} (McCarthy & Jarvis, 2010) so called factors are counted. Each factor is a subsequent stream of 
#'		tokens which ends (and is then counted as a full factor) when the TTR value falls below the given factor size. The value of
#'		remaining partial factors is estimated by the ratio of their current TTR to the factor size threshold. The MTLD is the total number 
#'		of tokens divided by the number of factors. The procedure is done twice, both forward and backward for all tokens, and the mean of 
#'		both calculations is the final MTLD result.
#'
#'		Wrapper function: \code{\link[koRpus:MTLD]{MTLD}}}
#'	\item{\code{"HD-D"}:}{The \emph{HD-D} value can be interpreted as the idealized version of \emph{vocd-D} (see McCarthy & Jarvis, 2007). For each type,
#'		the probability is computed (using the hypergeometric distribution) of drawing it at least one time when drawing randomly a certain
#'		number of tokens from the text -- 42 by default. The sum of these probabilities make up the HD-D value. The sum of probabilities relative to
#'		the drawn sample size (ATTR) is also reported.
#'
#'		Wrapper function: \code{\link[koRpus:HDD]{HDD}}}
#' }
#'
#' By default, if the text has to be tagged yet, the language definition is queried by calling \code{get.kRp.env(lang=TRUE)} 
#' internally.
#' Or, if \code{txt} has already been tagged, by default the language definition of that tagged object is read
#' and used. Set \code{force.lang=get.kRp.env(lang=TRUE)} or to any other valid value, if you want to forcibly overwrite this
#' default behaviour, and only then. See \code{\link[koRpus:kRp.POS.tags]{kRp.POS.tags}} for all supported languages.
#'
#' @param txt An object of either class \code{\link[koRpus]{kRp.tagged-class}}, \code{\link[koRpus]{kRp.txt.freq-class}},
#'		\code{\link[koRpus]{kRp.analysis-class}} or  \code{\link[koRpus]{kRp.txt.trans-class}}, containing the tagged text to be analyzed.
#' @param segment An integer value for MSTTR, defining how many tokens should form one segment.
#' @param factor.size A real number between 0 and 1, defining the MTLD factor size.
#' @param rand.sample An integer value, how many tokens should be assumed to be drawn for calculating HD-D.
#' @param window An integer value for MATTR, defining how many tokens the moving window should include.
#' @param case.sens Logical, whether types should be counted case sensitive.
#' @param lemmatize Logical, whether analysis should be carried out on the lemmatized tokens rather than all running word forms.
#' @param measure A character vector defining the measures which should be calculated. Valid elements are "TTR", "MSTTR", "MATTR", "C", "R", 
#		"CTTR", "U", "S", "K", "Maas", "HD-D" and "MTLD".
#' @param char A character vector defining whether data for plotting characteristic curves should be calculated. Valid elements are 
#'		"TTR","MATTR", "C", "R", "CTTR", "U", "S", "K", "Maas", "HD-D" and "MTLD".
#' @param char.steps An integer value defining the stepwidth for characteristic curves, in tokens.
#' @param force.lang A character string defining the language to be assumed for the text, by force. See details.
#' @param keep.tokens Logical. If \code{TRUE} all raw tokens and types will be preserved in the resulting object, in a slot called 
#'		\code{tt}. For the types, also their frequency in the analyzed text will be listed.
#' @param corp.rm.class A character vector with word classes which should be dropped. The default value
#'		\code{"nonpunct"} has special meaning and will cause the result of
#'		\code{kRp.POS.tags(lang, c("punct","sentc"), list.classes=TRUE)} to be used.
#' @param corp.rm.tag A character vector with POS tags which should be dropped.
#' @param quiet Logical. If \code{FALSE}, short status messages will be shown.
#' @return An object of class \code{\link[koRpus]{kRp.TTR-class}}.
#' @author m.eik michalke \email{meik.michalke@@hhu.de}
#' @keywords LD
#' @seealso \code{\link[koRpus:kRp.POS.tags]{kRp.POS.tags}},
#'		\code{\link[koRpus]{kRp.tagged-class}}, \code{\link[koRpus]{kRp.TTR-class}}
#' @references
#'		Covington, M.A. & McFall, J.D. (2010). Cutting the Gordian Knot: The Moving-Average Type-Token Ratio (MATTR). 
#'			\emph{Journal of Quantitative Linguistics}, 17(2), 94--100.
#'
#'		Maas, H.-D., (1972). Über den Zusammenhang zwischen Wortschatzumfang und Länge eines Textes. \emph{Zeitschrift für 
#'			Literaturwissenschaft und Linguistik}, 2(8), 73--96.
#'
#' 	McCarthy, P.M. & Jarvis, S. (2007). vocd: A theoretical and empirical evaluation. \emph{Language Testing}, 24(4), 459--488.
#'
#'		McCarthy, P.M. & Jarvis, S. (2010). MTLD, vocd-D, and HD-D: A validation study of sophisticated approaces to lexical diversity 
#'			assessment. \emph{Behaviour Research Methods}, 42(2), 381--392.
#'
#'		Tweedie. F.J. & Baayen, R.H. (1998). How Variable May a Constant Be? Measures of Lexical Richness in Perspective.
#' 		\emph{Computers and the Humanities}, 32(5), 323--352.
#' @export
#' @examples
#' \dontrun{
#' lex.div(tagged.text)
#' }
lex.div <- function(txt, segment=100, factor.size=0.72, rand.sample=42, window=100,
		case.sens=FALSE, lemmatize=FALSE,
		measure=c("TTR","MSTTR","MATTR","C","R","CTTR","U","S","K","Maas","HD-D","MTLD"),
		char=c("TTR","MATTR","C","R","CTTR","U","S","K","Maas","HD-D","MTLD"),
		char.steps=5,
		force.lang=NULL,
		keep.tokens=FALSE,
		corp.rm.class="nonpunct",
		corp.rm.tag=c(), quiet=FALSE){

## TODO:
# - Tuldava (1997): T = LogLog(tokens)/LogLog((tokens/types)+A)^5
#   (was ist A?)
# - Zipf's Z
# - check MTLD charactersitics -- can't this be simplified to save a lot of time?

	## TODO: validation
	# the following implementations have already been checked against various tools
	# to validate the correctness of calculation. this doesn't mean they always came to identical
	# results at once, since the accuracy of input data (like number of tokens) might vary.
	# but if these differences were manually corrected, the results were similar/identical:
	# - C                    [AYG]
	# - CTTR                 [AYG]
	# - HD-D                 [JMC]
	# - Maas                 [MAS]
	# - MSTTR                [AYG]
	# - MTLD                 [JMC]
	# - R                    [AYG]
	# - TTR                  [TAL]
	# - U                    [AYG]
	# 
	# these measures produce plausible results, but need checking:
	# - MATTR
	# - S
	# - K
	# 
	# tools used:
	# AYG: http://aihaiyang.com/synlex/lexical
	# TAL: http://www.textalyser.net
	# 
	# other:
	# JMC: re-calculations by jarvis & mccarthy (thanks!!!)
	# MAS: example data in the original paper by Maas

	if(!is.numeric(factor.size) | factor.size > 1 | factor.size < 0){
		stop(simpleError(paste("Invalid factor size value (must be 0 < factor.size < 1):",factor.size)))
	} else {}
	if(!is.numeric(segment) | segment < 1){
		stop(simpleError(paste("Invalid segment value (must be > 0):",factor.size)))
	} else {}
	# check for optional measures
	if(!any(measure %in% c("TTR","MSTTR","MATTR","C","R","CTTR","U","S","K","Maas","HD-D","MTLD")) &
		!any(char %in% c("TTR","MATTR","C","R","CTTR","U","S","K","Maas","HD-D","MTLD"))){
			stop(simpleError(paste("You didn't specify at least one valid measure or characteristic!")))
	} else {}

	# get class kRp.tagged from txt object
	# the internal function tag.kRp.txt() will return the object unchanged if it
	# is already tagged, so it's safe to call it with the lang set here
	tagged.text <- tag.kRp.txt(txt, lang=force.lang, objects.only=TRUE)
	# set the language definition
	lang <- language.setting(tagged.text, force.lang)
	if(!isTRUE(quiet)){
		cat("Language: \"",lang,"\"\n", sep="")
	} else {}

	if(identical(corp.rm.class, "nonpunct")){
		corp.rm.class <- kRp.POS.tags(lang, tags=c("punct","sentc"), list.classes=TRUE)
	} else {}

	# calling internal function tagged.txt.rm.classes()
	txt.all.clean <- tagged.txt.rm.classes(tagged.text@TT.res,
			lemma=lemmatize, lang,
			corp.rm.class=corp.rm.class,
			corp.rm.tag=corp.rm.tag,
			as.vector=FALSE)
	txt.all.tokens <- txt.all.clean[,"token"]
	txt.type.freq <- type.freq(txt.all.clean, case.sens=case.sens)
	txt.all.types <- txt.type.freq[,"type"]
	if(!isTRUE(case.sens)){
		txt.all.tokens <- tolower(txt.all.tokens)
	} else {}
	num.all.tokens <- length(txt.all.tokens)
	num.all.types <- length(txt.all.types)
	# global value for steps if char=c(something)
	num.all.steps <- num.all.tokens %/% char.steps

	# some sanity checks
	if(num.all.tokens < 100){
		warning("Text is relatively short (<100 tokens), results are probably not reliable!")
	} else {}

	# function to calculate Yule's K
	k.calc <- function(txt.tokens){
		N <- length(txt.tokens)
		txt.types <- unique(txt.tokens)
		# first analize types for their frequencies
		type.freqs <- sapply(txt.types, function(x){
				return(sum(match(txt.tokens, x), na.rm=TRUE))
			})
		# now count the frequencies of frequencies
		freq.freqs <- t(sapply(unique(type.freqs), function(x){
				frq.frq <- sum(match(type.freqs, x), na.rm=TRUE)
				return(c(X=x, fx=frq.frq))
			}))
		# calculate the sum of all fx * X^2
		freq.sum <- sum(freq.freqs[, "X"]^2 * freq.freqs[, "fx"])

		# fill in the whole equasion
		result <- 1e4 * (freq.sum - N) / N^2

		return(result)
	}

	# function to calculate HD-D
	hdd.calc <- function(hdd.tokens, drawn){
		hdd.types <- unique(hdd.tokens)
		num.hdd.tokens <- length(hdd.tokens)
		num.hdd.types <- length(hdd.types)
		# get probability from hypergeometric distribution for each type
		hdd.type.probs <- sapply(hdd.types, function(x){
				num.types.in.tokens <- sum(match(hdd.tokens, x), na.rm=TRUE)
				num.nontypes <- num.hdd.tokens - num.types.in.tokens
				# probability of having the type at least once is the inverse of not drawing it at all
				# if 'drawn' is larger than number of tokens, set probability to 1
				if(drawn > num.hdd.tokens){
					type.prob <- 1
				} else {
					type.prob <- 1 - dhyper(0, num.types.in.tokens, num.nontypes, drawn)
				}
				return(type.prob)
			})
		summary.probs <- summary(hdd.type.probs)
		sd.probs <- sd(hdd.type.probs)
		hdd.result <- sum(hdd.type.probs)
		hdd.ATTR <- hdd.result / drawn
		results <- list(HDD=hdd.result, ATTR=hdd.ATTR, type.probs=sort(hdd.type.probs, decreasing=TRUE), summary=summary.probs, sd=sd.probs)
		return(results)
	}

	# initialize result object
	lex.div.results <- new("kRp.TTR")


	###################################
	## diversity measures start here ##
	###################################


	## calculate TTR
	if("TTR" %in% measure){
		lex.div.results@TTR <- ttr.calc(num.tokens=num.all.tokens, num.types=num.all.types, type="TTR")
	} else {}

	## calculate Herdan's C: log(types) / log(tokens)
	if("C" %in% measure){
		lex.div.results@C.ld <- ttr.calc(num.tokens=num.all.tokens, num.types=num.all.types, type="C")
	} else {}

	## calculate Guiraud's R: types / sqrt(tokens)
	if("R" %in% measure){
		lex.div.results@R.ld <- ttr.calc(num.tokens=num.all.tokens, num.types=num.all.types, type="R")
	} else {}

	## calculate Carroll's CTTR: types / 2*sqrt(tokens)
	if("CTTR" %in% measure){
		lex.div.results@CTTR <- ttr.calc(num.tokens=num.all.tokens, num.types=num.all.types, type="CTTR")
	} else {}

	## calculate Uber Index U: (log(tokens))^2 / (log(tokens) - log(types)) 
	if("U" %in% measure){
		lex.div.results@U.ld <- ttr.calc(num.tokens=num.all.tokens, num.types=num.all.types, type="U")
	} else {}

	## calculate Summer's S: LogLog(types) / LogLog(tokens)
	if("S" %in% measure){
		lex.div.results@S.ld <- ttr.calc(num.tokens=num.all.tokens, num.types=num.all.types, type="S")
	} else {}

	## calculate Maas' a and lgV0 indices
	if("Maas" %in% measure){
		lex.div.results@Maas <- ttr.calc(num.tokens=num.all.tokens, num.types=num.all.types, type="Maas")
		lex.div.results@lgV0 <- lgV0.calc(num.tokens=num.all.tokens, num.types=num.all.types, x=0)
		lex.div.results@lgeV0 <- lgV0.calc(num.tokens=num.all.tokens, num.types=num.all.types, x=0, log.base=exp(1))
		# calculate relative lexical growth, using first half of the text and full text
		Maas.txt.half <- txt.all.tokens[1:(num.all.tokens %/% 2)]
		Maas.num.tokens.half <- length(Maas.txt.half)
		Maas.num.types.half <- length(unique(Maas.txt.half))
		lex.div.results@Maas.grw <- lex.growth(N1=Maas.num.tokens.half, V1=Maas.num.types.half, N2=num.all.tokens, V2=num.all.types)
	} else {}

	## calculate MSTTR
	if("MSTTR" %in% measure){
		if(num.all.tokens < segment){
			warning(paste("MSTTR: Skipped calculation, segment size is ", segment, ", but the text has only ", num.all.tokens, " tokens!", sep=""))
		} else {
			full.iter <- num.all.tokens %/% segment
			msttr.dropped <- num.all.tokens %% segment
			# iterate over all tokens
			msttr.iters <- sapply(0:(full.iter-1), function(x){
												segm.start <- (x*segment)+1
												segm.end <- (x*segment)+segment
												ttr <- ttr.calc(txt.tokens=txt.all.tokens[segm.start:segm.end])
												return(ttr)
											})
			msttr.res <- mean(msttr.iters)
			msttr.sd <- sd(msttr.iters)
			lex.div.results@MSTTR <- list(MSTTR=msttr.res, TTR.seg=msttr.iters, dropped=msttr.dropped, sd=msttr.sd)
		}
	} else {}

	## calculate MATTR
	# needed also for characteristics
	if(any(c("MATTR", "MATTR.char") %in% measure)){
		# check if there are less tokens than window size
		if(num.all.tokens <= window){
			warning(paste("MATTR: Skipped calculation, window size is ", window, ", but the text has only ", num.all.tokens, " tokens!", sep=""))
		} else {
			mattr.list <- list()
			# take the first n tokens
			mattr.win.tokens <- txt.all.tokens[1:window]
			# fill the initial type list
			for (this.token in mattr.win.tokens){
				mattr.list[[this.token]] <- list.add.type(this.token=this.token, type.list=mattr.list)
			}

			# initialize vector with TTRs
			mattr.all.TTR <- c(length(mattr.list) / window)
			# give the entry the name of the last token
			names(mattr.all.TTR) <- txt.all.tokens[window]

			# now move through the remaining text.
			# at each step, drop the first token and add the next unused one
			nextToken <- window + 1
			while (nextToken <= num.all.tokens) {
				# remove the first token
				mattr.list[[txt.all.tokens[nextToken - window]]] <- list.drop.type(this.token=txt.all.tokens[nextToken - window], type.list=mattr.list)
				# add the next one
				mattr.list[[txt.all.tokens[nextToken]]] <- list.add.type(this.token=txt.all.tokens[nextToken], type.list=mattr.list)
				# calculate new TTR
				mattr.all.TTR <- c(mattr.all.TTR, length(mattr.list) / window)
				names(mattr.all.TTR)[length(mattr.all.TTR)] <- txt.all.tokens[nextToken]
				# prepare for next round
				nextToken <- nextToken + 1
			}

			lex.div.results@MATTR <- list(MATTR=mean(mattr.all.TTR), TTR.win=mattr.all.TTR, sd=sd(mattr.all.TTR))
		}
	} else {}

	## Yule's K, frequency correction
	if("K" %in% measure){
		lex.div.results@K.ld <- k.calc(txt.all.tokens)
	} else {}

	## calculate HD-D
	if("HD-D" %in% measure){
		lex.div.results@HDD <- hdd.calc(txt.all.tokens, drawn=rand.sample)
	} else {}

	## calculate MTLD
	if("MTLD" %in% measure | "MTLD" %in% char){
		# implemented as a function, to be re-used for characteristic data calculation
		# to get only the backwards data (used for characteristics), set "back.only=TRUE)"
		mtld.calc <- function(mtld.txt, back.only=FALSE){
			mtld.num.tokens <- length(mtld.txt)
			# make this another function, to use it forward and backward
			mtld.sub.calc <- function(mtld.sub.txt){
				# initialize values and start at first word
				# factors begin with 0,division by zero might occur?!
				mtld.factors <- 0
				# subfactors begin with 0, since there is no rest when you've just started
				mtld.sub.factors <- 0
				mtld.ttr <- 1
				mtld.start <- 1
				mtld.words <- 2
				mtld.last.factor <- FALSE
				# initialize list of types
				mtld.type.list <- list()
				mtld.type.list[[mtld.sub.txt[1]]] <- 1
				mtld.all.results <- data.frame(start=NA, end=NA, token=NA, TTR=NA, factors=NA, stringsAsFactors=FALSE)[-1,]

				while((mtld.start + 1) < mtld.num.tokens){
					while(mtld.ttr > factor.size){
						# update the type list
						mtld.type.list[[mtld.sub.txt[mtld.words]]] <- list.add.type(this.token=mtld.sub.txt[mtld.words], type.list=mtld.type.list)
						mtld.ttr <- length(mtld.type.list) / (mtld.words - mtld.start + 1)
							# did the last token make this a full factor?
							if(mtld.ttr > factor.size){
								# if this is the case, calculate a partial factor value to add
								mtld.sub.factors <- mtld.factors + ((1 - mtld.ttr) / (1 - factor.size))
							} else {
								mtld.sub.factors <- mtld.factors + 1
							}
						mtld.all.results <- rbind(mtld.all.results, data.frame(
							start=mtld.start,
							end=mtld.words,
							token=mtld.sub.txt[mtld.words],
							TTR=mtld.ttr,
							factors=mtld.sub.factors,
							stringsAsFactors=FALSE))
						# see if we reached the end of tokens
						if(mtld.words == mtld.num.tokens){
							mtld.last.factor <- TRUE
							break
						} else {}
						mtld.words <- mtld.words + 1
					}

					# check if we need to increase the factor value
					if(!isTRUE(mtld.last.factor)){
						mtld.factors <- mtld.factors + 1
					} else {
						mtld.factors <- mtld.sub.factors
					}

					# set values to go on from here
					mtld.start <- mtld.words
					mtld.words <- mtld.start + 1
					# reset TTR for next round
					mtld.ttr <- 1
					# reset list of types
					mtld.type.list <- list()
					mtld.type.list[[mtld.sub.txt[mtld.start]]] <- 1
				}
				# individual factor lengths
				mtld.factor.starts <- unique(mtld.all.results[["start"]])
				mtld.factor.lengths <- c()
				mtld.factor.lengths.complete <- c()
				for (this.start in mtld.factor.starts){
					this.end <- max(mtld.all.results[["end"]][mtld.all.results[["start"]] == this.start])
					mtld.factor.lengths[length(mtld.factor.lengths) + 1]  <- this.end - this.start + 1
					# only complete factors
					if(mtld.all.results[["TTR"]][mtld.all.results[["end"]] == this.end] <= factor.size){
						mtld.factor.lengths.complete[length(mtld.factor.lengths.complete) + 1]  <- this.end - this.start + 1
					} else {}
				}
				# check if any full factors appeared at all
				if(is.null(mtld.factor.lengths.complete)){
					mtld.factor.lengths.complete <- 0
				} else {}

				mtld.results <- list(
					MTLD.all=mtld.all.results,
					factors=mtld.factors,
					lengths=mtld.factor.lengths,
					lengths.complete=mtld.factor.lengths.complete)
			} # end function mtld.sub.calc()

			if(isTRUE(back.only)){
				mtld.results <- mtld.sub.calc(rev(mtld.txt))[["MTLD.all"]]
			} else {
				mtld.res.forw <- mtld.sub.calc(mtld.txt)
				mtld.res.back <- mtld.sub.calc(rev(mtld.txt))
				mtld.res.mean <- mean(c(mtld.res.forw[["factors"]], mtld.res.back[["factors"]]))
				mtld.len.mean <- mean(c(mtld.res.forw[["lengths"]], mtld.res.back[["lengths"]]))
				mtld.len.sd <- sd(c(mtld.res.forw[["lengths"]], mtld.res.back[["lengths"]]))
				mtld.len.mean.cmp <- mean(c(mtld.res.forw[["lengths.complete"]], mtld.res.back[["lengths.complete"]]))
				mtld.len.sd.cmp <- sd(c(mtld.res.forw[["lengths.complete"]], mtld.res.back[["lengths.complete"]]))
				# this is the final MTLD value
				mtld.res.value <- mtld.num.tokens / mtld.res.mean
				mtld.results <- list(
					MTLD=mtld.res.value,
					all.forw=mtld.res.forw[["MTLD.all"]],
					all.back=mtld.res.back[["MTLD.all"]],
					factors=c(forw=mtld.res.forw[["factors"]], mean=mtld.res.mean, back=mtld.res.back[["factors"]]),
					lengths=list(
						forw=mtld.res.forw[["lengths"]],
						forw.compl=mtld.res.forw[["lengths.complete"]],
						mean=mtld.len.mean,
						mean.compl=mtld.len.mean.cmp,
						sd=mtld.len.sd,
						sd.compl=mtld.len.sd.cmp,
						back=mtld.res.back[["lengths"]],
						back.compl=mtld.res.back[["lengths.complete"]]
					)
				)
			}
			return(mtld.results)
		} # end function mtld.calc()
	} else {}

	if(!is.na(match("MTLD", measure))){
		lex.div.results@MTLD <- mtld.calc(txt.all.tokens)
	} else {}

	## calculate TTR, C, R, CTTR, U, S and Maas characteristics
	# set up the base function
	ttr.calc.chars <- function(txt.tokens, type="TTR"){
		if(!isTRUE(quiet)){
			message(paste(type, ".char: Calculate ",type," values", sep=""))
		} else {}
		char.results <- t(sapply(1:num.all.steps, function(x){
					curr.token <- x * char.steps
					if(!isTRUE(quiet)){
						# just some feedback, so we know the machine didn't just freeze...
						if(identical(curr.token %% 50, 0)){
							message(paste(" reached token 1 to ", curr.token,"...", sep=""))
						} else {}
					} else {}
					char.temp <- c(token=curr.token, value=ttr.calc(txt.tokens=txt.all.tokens[1:curr.token], type=type))
					return(char.temp)
				}
			))
	}

	# do the actual calculations
	if("TTR" %in% char){
		lex.div.results@TTR.char <- ttr.calc.chars(txt.all.tokens, type="TTR")
	} else {}

	if("C" %in% char){
		lex.div.results@C.char <- ttr.calc.chars(txt.all.tokens, type="C")
	} else {}

	if("R" %in% char){
		lex.div.results@R.char <- ttr.calc.chars(txt.all.tokens, type="R")
	} else {}

	if("CTTR" %in% char){
		lex.div.results@CTTR.char <- ttr.calc.chars(txt.all.tokens, type="CTTR")
	} else {}

	if("U" %in% char){
		lex.div.results@U.char <- ttr.calc.chars(txt.all.tokens, type="U")
	} else {}

	if("S" %in% char){
		lex.div.results@S.char <- ttr.calc.chars(txt.all.tokens, type="S")
	} else {}

	if("MATTR" %in% char && num.all.tokens > window){
		# mattr.all.TTR should be available, otherwise try
		# lex.div.results@MATTR$TTR.win
		# characteristics are just the progressing mean of these TTRs
		mattr.num.TTRs <- length(mattr.all.TTR)
		mattr.num.all.steps <- mattr.num.TTRs %/% char.steps
		lex.div.results@MATTR.char <- t(sapply(1:mattr.num.all.steps, function(x){
				mattr.curr.token <- x * char.steps
				mattr.char.temp <- c(token=mattr.curr.token + window, value=mean(mattr.all.TTR[1:mattr.curr.token]))
				return(mattr.char.temp)
			}))
	} else {}

	if("Maas" %in% char){
		lex.div.results@Maas.char <- ttr.calc.chars(txt.all.tokens, type="Maas")
		maas.lgV.chars <- function(base){
			lgV0.char.res <- t(sapply(1:num.all.steps, function(x){
						curr.token <- x * char.steps
						if(!isTRUE(quiet)){
							# just some feedback, so we know the machine didn't just freeze...
							if(identical(curr.token %% 50, 0)){
								message(paste(" reached token 1 to ", curr.token,"...", sep=""))
							} else {}
						} else {}
						lgV0.char.temp <- c(token=curr.token, value=lgV0.calc(txt.all.tokens[1:curr.token], x=0, log.base=base))
						return(lgV0.char.temp)
					}
				))
			return(lgV0.char.res)
		}
		if(!isTRUE(quiet)){
			message("lgV0.char: Calculate lgV0 values")
		} else {}
		lex.div.results@lgV0.char <- maas.lgV.chars(base=10)
		if(!isTRUE(quiet)){
			message("lgeV0.char: Calculate lgeV0 values")
		} else {}
		lex.div.results@lgeV0.char <- maas.lgV.chars(base=exp(1))
	} else {}

	## calculate K characteristics
	if("K" %in% char){
		if(!isTRUE(quiet)){
			message("K.char: Calculate K values")
		} else {}
		lex.div.results@K.char <- t(sapply(1:num.all.steps, function(x){
					curr.token <- x * char.steps
					if(!isTRUE(quiet)){
						# just some feedback, so we know the machine didn't just freeze...
						if(identical(curr.token %% 50, 0)){
							message(paste(" reached token 1 to ", curr.token,"...", sep=""))
						} else {}
					} else {}
					k.char.temp <- c(token=curr.token, value=k.calc(txt.all.tokens[1:curr.token]))
					return(k.char.temp)
				}
			))
	} else {}

	## calculate HD-D characteristics
	if("HD-D" %in% char){
		if(!isTRUE(quiet)){
			message("HDD.char: Calculate HD-D values")
		} else {}
		lex.div.results@HDD.char <- t(sapply(1:num.all.steps, function(x){
					curr.token <- x * char.steps
					if(!isTRUE(quiet)){
						# just some feedback, so we know the machine didn't just freeze...
						if(identical(curr.token %% 50, 0)){
							message(paste(" reached token 1 to ", curr.token,"...", sep=""))
						} else {}
					} else {}
					hdd.value <- hdd.calc(txt.all.tokens[1:curr.token], drawn=rand.sample)[["HDD"]]
					hdd.char.temp <- c(token=curr.token, value=hdd.value)
					return(hdd.char.temp)
				}
			))
	} else {}

	## calculate MTLD characteristics
	# this can probably be be much faster, as actually only all.back needs to be recalculated in the loop
	if("MTLD" %in% char){
		if(!isTRUE(quiet)){
			message("MTLD.char: Calculate MTLD values")
		} else {}
		# if MTLD results have not yet been calculated, do it now to get the full "all.forw" data
		if(is.na(match("MTLD", measure))){
			MTLD <- mtld.calc(txt.all.tokens)
		} else {}
		mtld.char.forw <- lex.div.results@MTLD[["all.forw"]]
# 		MTLD.char.results <- t(sapply(1:num.all.steps, function(x){
# 					curr.token <- x * char.steps
# 					if(!isTRUE(quiet)){
# 						# just some feedback, so we know the machine didn't just freeze...
# 						if(identical(curr.token %% 50, 0)){
# 							message(paste(" reached token 1 to ", curr.token,"...", sep=""))
# 						} else {}
# 					} else {}
# 					mtld.char.temp <- c(token=curr.token, value=mtld.calc(txt.all.tokens[1:curr.token])[["MTLD"]])
# 					return(mtld.char.temp)
# 				}
# 			))
		lex.div.results@MTLD.char <- t(sapply(1:num.all.steps, function(x){
					curr.token <- x * char.steps
					if(!isTRUE(quiet)){
						# just some feedback, so we know the machine didn't just freeze...
						if(identical(curr.token %% 50, 0)){
							message(paste(" reached token 1 to ", curr.token,"...", sep=""))
						} else {}
					} else {}
					mtld.char.back <- mtld.calc(txt.all.tokens[1:curr.token], back.only=TRUE)
					# after the iteration process, line numbers are not identical to token numbers
					mtld.back.value <- mtld.char.back[which(mtld.char.back[["end"]] == curr.token), "factors"]
					mtld.forw.value <- mtld.char.forw[which(mtld.char.forw[["end"]] == curr.token), "factors"]
					mtld.char.mean <- mean(c(mtld.forw.value, mtld.back.value))
					# uncomment to debug:
					# print(paste("token: ", curr.token, "(", txt.all.tokens[curr.token],") forw: ", mtld.forw.value, "back: ", mtld.back.value, " -- MTLD: ", mtld.char.mean, sep=""))
					mtld.char.value <- curr.token / mtld.char.mean
					mtld.char.temp <- c(token=curr.token, value=mtld.char.value)
					return(mtld.char.temp)
				}
			))
	} else {}

	lex.div.results@param <- list(segment=segment, factor.size=factor.size, rand.sample=rand.sample, case.sens=case.sens, lemmatize=lemmatize)

	# keep raw text material only if explicitly told so
	if(isTRUE(keep.tokens)){
		lex.div.results@tt <- list(tokens=txt.all.tokens, types=txt.type.freq, num.tokens=num.all.tokens, num.types=num.all.types)
	} else {
		lex.div.results@tt <- list(tokens=character(), types=character(), num.tokens=num.all.tokens, num.types=num.all.types)
	}

	## for the time being, give a warning until all implementations have been validated
	needs.warning <- measure %in% c("MATTR","S","K")
	if(any(needs.warning)){
		warning(paste("Note: The implementations of these formulas are still subject to validation:\n  ",
		paste(measure[needs.warning], collapse=", "),
		"\n  Use the results with caution, even if they seem plausible!", sep=""), call.=FALSE)
	} else {}
	return(lex.div.results)
}
